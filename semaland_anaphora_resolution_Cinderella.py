#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import print_function, with_statement, division

"""
cosi299a- Cinderella
alexluu@brandeis.edu

Demo:
>>> 
END OF FILE: amr_edited_ariel.txt
END OF FILE: amr_edited_aurora.txt
END OF FILE: amr_edited_belle.txt
END OF FILE: amr_edited_cinderella.txt
END OF FILE: amr_edited_snow_white.txt
Cache size: 1
Ariel
0 	 {}
1 	 {}
2 	 {'x1': [(1.0, 0, 'x10')]}
3 	 {'x4': [(1.0, 1, 'x10')]}
4 	 {}
5 	 {}
6 	 {'x1': [(0.5, 5, 'x3'), (0.5, 5, 'x5')]}
7 	 {}
8 	 {}
9 	 {'x2': [(1.0, 7, 'x5')], 'x8': [(1.0, 7, 'x10')]}
10 	 {}
11 	 {'x3': [(1.0, 3, 'x4')], 'x1': [(1.0, 5, 'x1')]}
12 	 {}
13 	 {'x1': [(1.0, 11, 'x1')]}
14 	 {'x3': [(1.0, 3, 'x12')]}
15 	 {'x2': [(1.0, 13, 'x4')], 'x8': [(1.0, 9, 'x2')]}
16 	 {'x8': [(1.0, 15, 'x8')], 'x13': [(1.0, 15, 'x20')]}
17 	 {}
18 	 {'x7': [(1.0, 6, 'x3')], 'x4': [(1.0, 13, 'x1')]}
19 	 {'x8': [(1.0, 18, 'x4')], 'x20': [(1.0, 0, 'x8')], 'x4': [(1.0, 15, 'x2')]}
20 	 {'x10': [(1.0, 16, 'x18')]}
21 	 {'x10': [(1.0, 19, 'x8')], 'x12': [(1.0, 1, 'x16')], 'x7': [(1.0, 16, 'x4')]}
22 	 {'x2': [(1.0, 1, 'x2')], 'x18': [(1.0, 19, 'x7')], 'x1': [(1.0, 21, 'x10')], 'x6': [(1.0, 19, 'x3')]}
23 	 {'x10': [(1.0, 16, 'x8')], 'x3': [(1.0, 21, 'x7')], 'x12': [(1.0, 17, 'x7')]}
24 	 {}
25 	 {'x18': [(1.0, 19, 'x4')], 'x8': [(1.0, 16, 'x13')], 'x3': [(1.0, 23, 'x18')], 'x12': [(1.0, 20, 'x10')], 'x13': [(1.0, 17, 'x8')], 'x15': [(1.0, 17, 'x10')]}
26 	 {'x6': [(1.0, 24, 'x10')]}
27 	 {'x1': [(1.0, 26, 'x1')], 'x6': [(1.0, 25, 'x3')]}
28 	 {'x1': [(1.0, 23, 'x10')], 'x6': [(1.0, 26, 'x6')]}
29 	 {}
30 	 {'x5': [(1.0, 25, 'x6')]}
31 	 {'x2': [(1.0, 8, 'x4')], 'x11': [(1.0, 11, 'x3')], 'x9': [(1.0, 28, 'x1')], 'x18': [(1.0, 11, 'x12')], 'x16': [(1.0, 25, 'x18')]}
32 	 {'x18': [(1.0, 25, 'x8')], 'x6': [(1.0, 1, 'x9')]}
33 	 {'x11': [(1.0, 32, 'x8')], 'x5': [(1.0, 25, 'x15')], 'x7': [(1.0, 23, 'x3')], 'x3': [(1.0, 23, 'x12')]}
34 	 {}
35 	 {'x10': [(1.0, 32, 'x10')]}
36 	 {'x12': [(1.0, 33, 'x3')], 'x4': [(1.0, 19, 'x20')]}
37 	 {'x9': [(1.0, 36, 'x12')]}
38 	 {'x10': [(1.0, 25, 'x11')], 'x14': [(1.0, 37, 'x9')]}
39 	 {'x2': [(1.0, 38, 'x14')], 'x18': [(1.0, 31, 'x18')], 'x12': [(1.0, 35, 'x9')], 'x10': [(1.0, 25, 'x11')]}
40 	 {'x9': [(1.0, 8, 'x10')], 'x1': [(1.0, 39, 'x12')], 'x7': [(1.0, 39, 'x2')]}
41 	 {'x7': [(1.0, 25, 'x12')]}
42 	 {'x2': [(1.0, 40, 'x7')], 'x9': [(1.0, 37, 'x12')]}
43 	 {'x10': [(1.0, 42, 'x9')], 'x4': [(1.0, 33, 'x17')], 'x17': [(1.0, 22, 'x9')]}
44 	 {'x3': [(1.0, 43, 'x6')], 'x1': [(1.0, 43, 'x2')], 'x14': [(1.0, 41, 'x6')]}
Number of nodes: 315
Number of edges: 277
Number of ancas: 140
Number of coref links: 74


Aurora
0 	 {}
1 	 {'x1': [(1.0, 0, 'x15')], 'x4': [(1.0, 0, 'x20')]}
2 	 {'x19': [(1.0, 0, 'x13')], 'x5': [(1.0, 0, 'x6')]}
3 	 {'x10': [(1.0, 2, 'x24')], 'x19': [(1.0, 2, 'x24')], 'x8': [(1.0, 2, 'x13')], 'x21': [(1.0, 2, 'x1')], 'x5': [(1.0, 2, 'x1')]}
4 	 {'x1': [(1.0, 2, 'x1')], 'x7': [(1.0, 3, 'x10')]}
5 	 {'x1': [(1.0, 4, 'x1')], 'x6': [(1.0, 3, 'x8')]}
6 	 {'x10': [(1.0, 2, 'x5')], 'x3': [(1.0, 1, 'x13')]}
7 	 {'x3': [(1.0, 5, 'x1')], 'x7': [(1.0, 4, 'x7')]}
8 	 {'x9': [(1.0, 5, 'x13')], 'x7': [(1.0, 2, 'x1')]}
9 	 {'x2': [(1.0, 6, 'x10')], 'x10': [(1.0, 6, 'x14')]}
10 	 {}
11 	 {'x7': [(1.0, 4, 'x11')], 'x4': [(1.0, 0, 'x9')], 'x17': [(1.0, 1, 'x4')]}
12 	 {'x18': [(1.0, 3, 'x12')], 'x13': [(1.0, 11, 'x7')], 'x5': [(1.0, 9, 'x2')]}
13 	 {'x21': [(1.0, 3, 'x15')], 'x12': [(1.0, 12, 'x18')], 'x6': [(1.0, 12, 'x23')]}
14 	 {'x6': [(1.0, 2, 'x1')]}
15 	 {'x7': [(1.0, 12, 'x25')]}
16 	 {'x19': [(1.0, 5, 'x6')], 'x9': [(1.0, 14, 'x6')]}
17 	 {'x8': [(1.0, 13, 'x15')]}
18 	 {}
19 	 {'x9': [(1.0, 14, 'x12')], 'x1': [(1.0, 16, 'x9')]}
20 	 {'x3': [(1.0, 19, 'x1')], 'x8': [(1.0, 18, 'x10')]}
21 	 {'x3': [(1.0, 2, 'x1')], 'x8': [(1.0, 20, 'x8')], 'x1': [(1.0, 20, 'x8')], 'x13': [(1.0, 12, 'x13')], 'x11': [(1.0, 11, 'x4')]}
22 	 {'x10': [(1.0, 16, 'x19')], 'x1': [(1.0, 2, 'x1')], 'x6': [(1.0, 20, 'x3')]}
23 	 {'x9': [(1.0, 3, 'x21')], 'x4': [(1.0, 12, 'x2')]}
24 	 {'x11': [(1.0, 22, 'x6')], 'x27': [(1.0, 21, 'x1')], 'x1': [(1.0, 22, 'x1')]}
25 	 {}
26 	 {'x10': [(1.0, 14, 'x3')], 'x1': [(1.0, 24, 'x11')], 'x14': [(1.0, 24, 'x1')]}
27 	 {}
28 	 {'x18': [(1.0, 21, 'x13')], 'x11': [(1.0, 26, 'x10')], 'x8': [(1.0, 26, 'x20')], 'x23': [(1.0, 22, 'x10')], 'x3': [(1.0, 26, 'x1')]}
29 	 {'x10': [(1.0, 28, 'x8')]}
30 	 {'x12': [(1.0, 21, 'x19')], 'x6': [(1.0, 24, 'x27')]}
31 	 {'x1': [(1.0, 28, 'x3')]}
32 	 {'x1': [(1.0, 31, 'x1')], 'x7': [(1.0, 24, 'x22')], 'x5': [(1.0, 30, 'x12')]}
33 	 {'x10': [(1.0, 28, 'x18')], 'x2': [(1.0, 12, 'x5')], 'x6': [(1.0, 29, 'x10')]}
34 	 {'x2': [(1.0, 4, 'x12')]}
35 	 {'x10': [(1.0, 28, 'x23')], 'x3': [(1.0, 33, 'x2')], 'x13': [(1.0, 6, 'x17')], 'x6': [(1.0, 8, 'x4')]}
36 	 {'x3': [(1.0, 35, 'x3')], 'x12': [(1.0, 10, 'x8')], 'x6': [(1.0, 9, 'x10')], 'x4': [(1.0, 9, 'x8')]}
37 	 {'x11': [(1.0, 26, 'x14')], 'x6': [(1.0, 36, 'x3')], 'x4': [(1.0, 33, 'x10')], 'x17': [(1.0, 34, 'x11')]}
38 	 {'x19': [(1.0, 36, 'x3')], 'x12': [(1.0, 8, 'x2')], 'x1': [(1.0, 37, 'x4')], 'x15': [(1.0, 36, 'x12')], 'x5': [(1.0, 35, 'x10')]}
39 	 {'x19': [(1.0, 27, 'x8')], 'x12': [(1.0, 13, 'x20')], 'x9': [(1.0, 27, 'x8')]}
40 	 {'x7': [(1.0, 38, 'x1')], 'x16': [(1.0, 32, 'x7')], 'x17': [(1.0, 39, 'x6')]}
41 	 {}
42 	 {'x2': [(1.0, 40, 'x7')]}
43 	 {'x12': [(1.0, 40, 'x17')], 'x9': [(1.0, 42, 'x2')]}
44 	 {}
45 	 {'x10': [(1.0, 37, 'x11')], 'x6': [(1.0, 26, 'x14')]}
46 	 {'x18': [(1.0, 42, 'x11')], 'x10': [(1.0, 44, 'x4')], 'x14': [(1.0, 39, 'x15')], 'x22': [(1.0, 34, 'x7')]}
47 	 {'x2': [(1.0, 41, 'x5')], 'x8': [(1.0, 43, 'x12')], 'x12': [(1.0, 4, 'x5')], 'x5': [(1.0, 46, 'x12')]}
48 	 {'x3': [(1.0, 47, 'x8')], 'x8': [(1.0, 37, 'x14')]}
49 	 {'x5': [(1.0, 40, 'x29')]}
50 	 {'x3': [(1.0, 40, 'x12')], 'x6': [(1.0, 46, 'x14')], 'x5': [(1.0, 38, 'x5')]}
Number of nodes: 416
Number of edges: 369
Number of ancas: 177
Number of coref links: 113


Belle
0 	 {}
1 	 {}
2 	 {'x3': [(1.0, 0, 'x11')], 'x20': [(1.0, 0, 'x15')]}
3 	 {}
4 	 {'x5': [(1.0, 3, 'x6')]}
5 	 {'x5': [(1.0, 4, 'x5')]}
6 	 {'x10': [(1.0, 5, 'x1')], 'x18': [(1.0, 0, 'x4')], 'x1': [(1.0, 5, 'x5')]}
7 	 {}
8 	 {}
9 	 {}
10 	 {'x9': [(1.0, 8, 'x1')]}
11 	 {}
12 	 {'x4': [(1.0, 2, 'x20')]}
13 	 {'x9': [(1.0, 11, 'x8')]}
14 	 {'x8': [(1.0, 12, 'x4')], 'x1': [(1.0, 13, 'x9')]}
15 	 {'x2': [(1.0, 14, 'x1')]}
16 	 {'x8': [(1.0, 15, 'x2')]}
17 	 {'x10': [(1.0, 16, 'x8')], 'x20': [(1.0, 16, 'x5')], 'x16': [(1.0, 3, 'x10')]}
18 	 {'x1': [(1.0, 17, 'x10')], 'x6': [(1.0, 16, 'x8')]}
19 	 {}
20 	 {'x2': [(1.0, 15, 'x11')]}
21 	 {'x6': [(1.0, 18, 'x1')]}
22 	 {'x2': [(1.0, 0, 'x4')], 'x6': [(1.0, 7, 'x8')]}
23 	 {'x13': [(1.0, 16, 'v')], 'x1': [(1.0, 22, 'x2')], 'x9': [(1.0, 21, 'x6')]}
24 	 {'x4': [(1.0, 23, 'x9')]}
25 	 {'x11': [(1.0, 24, 'x4')], 'x1': [(1.0, 24, 'x4')], 'x7': [(1.0, 23, 'x1')]}
26 	 {'x2': [(1.0, 16, 'x8')], 'x8': [(1.0, 16, 'x8')]}
27 	 {'x12': [(1.0, 16, 'x8')], 'x5': [(1.0, 25, 'x1')]}
28 	 {'x10': [(1.0, 24, 'x7')], 'x2': [(1.0, 27, 'x5')], 'x16': [(1.0, 1, 'x6')]}
29 	 {'x7': [(1.0, 24, 'x17')]}
30 	 {'x19': [(1.0, 17, 'x16')], 'x1': [(1.0, 28, 'x2')], 'x14': [(1.0, 28, 'x10')], 'x7': [(1.0, 17, 'x5')], 'x4': [(1.0, 21, 'x9')]}
31 	 {'x3': [(1.0, 0, 'x4')], 'x8': [(1.0, 30, 'x1')], 'x6': [(1.0, 18, 'x4')]}
32 	 {'x7': [(1.0, 31, 'x8')]}
33 	 {'x8': [(1.0, 32, 'x7')], 'x7': [(1.0, 32, 'x7')]}
34 	 {'x2': [(1.0, 31, 'x3')]}
35 	 {'x7': [(1.0, 33, 'x7')], 'x5': [(1.0, 31, 'x6')]}
36 	 {'x9': [(1.0, 33, 'x7')], 'x14': [(1.0, 4, 'x16')]}
37 	 {'x11': [(1.0, 17, 'x7')], 'x9': [(1.0, 36, 'x16')], 'x7': [(1.0, 35, 'x5')], 'x4': [(1.0, 36, 'x14')], 'x15': [(1.0, 30, 'x19')]}
38 	 {'x2': [(1.0, 33, 'x7')], 'x17': [(1.0, 37, 'x11')]}
39 	 {'x1': [(1.0, 38, 'x2')], 'x4': [(1.0, 32, 'x15')]}
40 	 {'x1': [(1.0, 37, 'x9')], 'x4': [(1.0, 39, 'x4')]}
41 	 {}
42 	 {'x3': [(1.0, 37, 'x4')]}
43 	 {'x8': [(1.0, 2, 'x13')]}
44 	 {}
45 	 {'x2': [(1.0, 40, 'x4')]}
46 	 {'x5': [(1.0, 2, 'x3')]}
47 	 {'x18': [(1.0, 46, 'x5')], 'x3': [(1.0, 22, 'x12')], 'x20': [(1.0, 40, 'x1')]}
Number of nodes: 317
Number of edges: 280
Number of ancas: 129
Number of coref links: 72


Cinderella
0 	 {}
1 	 {}
2 	 {'x1': [(1.0, 1, 'x10')], 'x7': [(1.0, 0, 'x10')]}
3 	 {'x1': [(1.0, 0, 'x12')]}
4 	 {'x1': [(1.0, 3, 'x1')], 'x6': [(1.0, 3, 'x1')]}
5 	 {}
6 	 {'x16': [(1.0, 4, 'x1')]}
7 	 {'x4': [(1.0, 6, 'x16')]}
8 	 {}
9 	 {}
10 	 {}
11 	 {'x3': [(1.0, 6, 'x20')], 'x8': [(1.0, 7, 'x4')]}
12 	 {'x1': [(1.0, 11, 'x8')], 'x7': [(1.0, 9, 'x8')]}
13 	 {'x5': [(1.0, 12, 'x1')]}
14 	 {'x8': [(1.0, 13, 'x5')]}
15 	 {'x11': [(1.0, 12, 'x7')], 'x1': [(1.0, 11, 'x8')], 'x6': [(1.0, 14, 'x8')]}
16 	 {'x9': [(1.0, 11, 'x8')], 'x1': [(1.0, 15, 'x1')], 'x7': [(1.0, 11, 'x8')]}
17 	 {}
18 	 {'x1': [(1.0, 16, 'x1')], 'x7': [(1.0, 7, 'x6')], 'x4': [(1.0, 11, 'x11')]}
19 	 {'x2': [(1.0, 15, 'x6')], 'x12': [(1.0, 13, 'x8')], 'x9': [(1.0, 2, 'x7')], 'x7': [(1.0, 2, 'x1')]}
20 	 {'x9': [(1.0, 3, 'x9')], 'x1': [(0.3333333333333333, 19, 'x2'), (0.3333333333333333, 19, 'x9'), (0.3333333333333333, 19, 'x7')]}
21 	 {'x13': [(1.0, 19, 'x12')], 'x9': [(1.0, 19, 'x12')], 'x5': [(1.0, 11, 'x2')]}
22 	 {'x8': [(1.0, 21, 'x9')], 'x1': [(1.0, 19, 'x2')]}
23 	 {'x3': [(1.0, 14, 'x9')], 'x7': [(1.0, 15, 'x11')]}
24 	 {}
25 	 {'x10': [(1.0, 19, 'x9')], 'x1': [(1.0, 22, 'x1')], 'x6': [(1.0, 11, 'x8')], 'x4': [(1.0, 11, 'x8')]}
26 	 {'x3': [(1.0, 20, 'x9')], 'x9': [(1.0, 20, 'x5')], 'x13': [(1.0, 22, 'x8')], 'x7': [(1.0, 20, 'x3')], 'x11': [(1.0, 25, 'x1')]}
27 	 {'x9': [(1.0, 26, 'x9')], 'x1': [(1.0, 26, 'x3')], 'x4': [(1.0, 26, 'x13')]}
28 	 {'x1': [(1.0, 18, 'x1')]}
29 	 {'x8': [(1.0, 23, 'x7')]}
30 	 {'x1': [(1.0, 29, 'x1')]}
31 	 {}
32 	 {}
33 	 {'x2': [(1.0, 23, 'x3')], 'x11': [(1.0, 27, 'x4')], 'x1': [(1.0, 30, 'x1')], 'x7': [(1.0, 29, 'x8')]}
34 	 {'x8': [(1.0, 31, 'x5')], 'x12': [(1.0, 32, 'x6')]}
35 	 {'x1': [(1.0, 33, 'x1')], 'x7': [(1.0, 33, 'x11')], 'x4': [(1.0, 33, 'x11')]}
36 	 {'x18': [(1.0, 0, 'x4')], 'x12': [(1.0, 0, 'x4')], 'x13': [(1.0, 11, 'x8')], 'x20': [(1.0, 35, 'x4')]}
37 	 {'x3': [(1.0, 33, 'x7')], 'x13': [(1.0, 18, 'x4')], 'x7': [(1.0, 33, 'x2')], 'x5': [(1.0, 16, 'x4')]}
38 	 {'x14': [(1.0, 25, 'x10')], 'x7': [(1.0, 37, 'x5')]}
39 	 {'x8': [(1.0, 29, 'x3')]}
40 	 {'x10': [(1.0, 36, 'x14')], 'x9': [(1.0, 11, 'x8')], 'x6': [(1.0, 36, 'x12')]}
41 	 {'x5': [(1.0, 35, 'x1')]}
42 	 {'x8': [(1.0, 38, 'x7')]}
43 	 {'x8': [(1.0, 38, 'x16')]}
44 	 {'x2': [(1.0, 41, 'x5')], 'x9': [(1.0, 35, 'x11')]}
45 	 {'x2': [(1.0, 42, 'x8')], 'x10': [(1.0, 38, 'x14')], 'x15': [(1.0, 44, 'x9')]}
46 	 {'x10': [(1.0, 44, 'x2')], 'x1': [(1.0, 28, 'x1')], 'x4': [(1.0, 44, 'x2')]}
47 	 {'x2': [(1.0, 11, 'x3')], 'x12': [(1.0, 45, 'x15')], 'x6': [(1.0, 45, 'x5')], 'x16': [(1.0, 46, 'x10')]}
48 	 {}
49 	 {'x1': [(1.0, 47, 'x16')], 'x7': [(1.0, 17, 'x4')], 'x4': [(1.0, 45, 'x2')]}
50 	 {'x8': [(1.0, 46, 'x13')], 'x13': [(1.0, 33, 'x11')]}
51 	 {'x8': [(1.0, 50, 'x5')], 'x5': [(1.0, 11, 'x8')]}
Number of nodes: 347
Number of edges: 301
Number of ancas: 169
Number of coref links: 95


Snow_White
0 	 {}
1 	 {}
2 	 {}
3 	 {}
4 	 {'x12': [(1.0, 3, 'x14')], 'x20': [(1.0, 0, 'x10')]}
5 	 {'x3': [(1.0, 4, 'x15')]}
6 	 {'x10': [(1.0, 4, 'x2')], 'x1': [(1.0, 5, 'x3')]}
7 	 {}
8 	 {}
9 	 {'x2': [(1.0, 4, 'x20')]}
10 	 {'x1': [(1.0, 9, 'x2')]}
11 	 {}
12 	 {'x7': [(1.0, 1, 'x8')]}
13 	 {'x14': [(1.0, 7, 'x2')]}
14 	 {}
15 	 {}
16 	 {'x14': [(1.0, 14, 'x8')], 'x5': [(1.0, 2, 'x4')]}
17 	 {'x8': [(1.0, 9, 'x9')]}
18 	 {'x9': [(1.0, 17, 'x8')], 'x1': [(0.5, 17, 'x8'), (0.5, 17, 'x5')], 'x12': [(1.0, 12, 'x6')]}
19 	 {'x10': [(1.0, 10, 'x1')], 'x3': [(1.0, 17, 'x5')], 'x13': [(1.0, 18, 'x6')]}
20 	 {'x3': [(1.0, 13, 'x14')]}
21 	 {'x8': [(1.0, 19, 'x10')], 'x12': [(1.0, 6, 'x10')], 'x16': [(1.0, 16, 'x5')], 'x17': [(1.0, 16, 'x5')]}
22 	 {'x6': [(1.0, 12, 'x7')], 'x7': [(1.0, 3, 'x3')]}
23 	 {'x7': [(1.0, 21, 'x12')]}
24 	 {}
25 	 {'x7': [(1.0, 21, 'x8')], 'x5': [(1.0, 24, 'x6')]}
26 	 {'x21': [(1.0, 4, 'x10')], 'x14': [(1.0, 23, 'x7')]}
27 	 {'x8': [(1.0, 26, 'x29')]}
28 	 {'x2': [(1.0, 26, 'x14')]}
29 	 {'x10': [(1.0, 28, 'x2')], 'x17': [(1.0, 11, 'x5')], 'x5': [(1.0, 19, 'x13')]}
30 	 {'x10': [(1.0, 29, 'x10')], 'x6': [(1.0, 29, 'x17')]}
31 	 {'x3': [(1.0, 23, 'x11')], 'x9': [(1.0, 27, 'x14')]}
32 	 {'x10': [(1.0, 22, 'x11')], 'x8': [(1.0, 26, 'x11')]}
33 	 {'x3': [(1.0, 31, 'x2')]}
34 	 {'x9': [(1.0, 25, 'x7')], 'x1': [(1.0, 33, 'x3')]}
35 	 {'x2': [(1.0, 33, 'x6')]}
36 	 {'x2': [(1.0, 0, 'x4')]}
37 	 {'x2': [(1.0, 31, 'x3')], 'x9': [(1.0, 19, 'x3')], 'x5': [(1.0, 21, 'x16')]}
Number of nodes: 250
Number of edges: 215
Number of ancas: 95
Number of coref links: 49


>>> 
"""

from constants import *
from semaland_utils import *
from semaland_amr_graph import *
from semaland_semantic_features import *
from semaland_string_features import *
import networkx as nx
import re
from collections import defaultdict
import xml.etree.ElementTree as ET
from operator import itemgetter
from copy import deepcopy

pros = dict()
for line in read_lines('Lists/pronouns.txt'):
    tokens = line.split()
    pro = dict()
    pro['num']=tokens[1]
    pro['gen']=tokens[2]
    pro['per']=tokens[3]
    pros[tokens[0]]=pro

def is_pronominal(g,n):
    """ 1/0 (True/False) """
    if g.node[n]['content'].ful_name_ in pros:
        return 1 # True
    return 0 # False

def is_pronominal_quote(g,n): # 1st and 2nd person pronouns
    """ 1/0 (True/False) """
    if g.node[n]['content'].ful_name_ in pros and \
       pros[g.node[n]['content'].ful_name_]['num'] in {1,2}:
        return 1 # True
    return 0 # False

def is_given(g,n): # node having at least one antecedent <- simplest
    """ 1/0 (True/False) """
##    if 'coref' in g.node[n]:
##        return 1 # True
    return 0 # False    

def classify_node(g,n): # g: AMR graph, n: AMR node
    """ -> '@', event, special concept, pronoun, conjunction, constant or other """
##    if n=='@':
##        return n
    if 'content' not in g.node[n]:
        return n
    node_concept = g.node[n]['content'].ful_name_
    if re.match(r'\S+-\d+',node_concept):
        return 'event' # node with sense tag
    if node_concept in AMR_SPECIAL_CONCEPTS:
        return 'special'
    if node_concept in NOMINATIVE_PRONOUNS:
        return 'pronoun'
    if node_concept in AMR_CONJUNCTIONS:
        return 'conjunction'
    if node_concept in DEIXIS:
        return 'deixis'
    if node_concept=='':
        return 'constant'
    return 'other' # named entities and abstract concepts???

def get_antecas(g): #antecas: antecedent candidates
    """ remove '@', events, special concepts, and constants """
    return set(n for n in g if classify_node(g,n) not in
               {'@','event','special','constant','conjunction','deixis'})

# assumption: the maximum number of arguments of a predicate < 10
l_re_in = r'^:ARG\d$'       # label pattern of core role in-edge
l_re_out = r'^:ARG\d-of$'   # label pattern of core role out-edge

def get_ancas(g): # g: AMR graph
    """ core roles or root node without sense tag """
    output = set()
    ns = set(n for n in g if classify_node(g,n) not in
             {'@','event','special','constant','deixis'})
    #for n in get_antecas(g):
    for n in ns:
        # root node, :ARGX, or :ARGX-of
        if '@' in g.predecessors(n) or \
           check_edge_label(g,n,'in',l_re_in) or \
           check_edge_label(g,n,'out',l_re_out): 
            output.add(n)
    for n in output:
        if is_pronominal_quote(g,n):
            output.remove(n)
    return output

def update_frequency(g,n,e_di,l_re): #assumption: existence of edges
    """ -> number of instances of each core role type a node plays """
    #output = defaultdict(int)
    output = dict()
    for e in get_edges_by_direction(g,n,e_di):
        if 'label' in e[2] and re.match(l_re,e[2]['label']):
            arg_index = e[2]['label'][4]
            if arg_index in output:
                output[arg_index] += 1
            else:
                output[arg_index] = 1
    return output

def update_frequency_ext(g,n,e_di,l_re,conj): # extended for conjunction case
    """ ... """
    if conj:
        output = dict()
        freq_n = update_frequency(g,n,e_di,l_re)
        keys_n = set(freq_n.keys())
        freq_conj = update_frequency(g,conj,e_di,l_re)
        keys_conj = set(freq_conj.keys())
        for i in keys_n.intersection(keys_conj):
            output[i] = freq_n[i] + freq_conj[i]
        for i in keys_n.difference(keys_conj):
            output[i] = freq_n[i]
        for i in keys_conj.difference(keys_n):
            output[i] = freq_conj[i]
        return output
    
    return update_frequency(g,n,e_di,l_re)

#def get_salience_factors(g,n):
def get_salience_factors(g,n,conj=None):
    # g: AMR graph, n: AMR node
    """ distance from graph root, core roles, pronominalization, giveness """
    output = dict()
    output['dis'] = nx.shortest_path_length(g,'@',n)
    #output['in'] = update_frequency(g,n,'in',l_re_in)
    output['in'] = update_frequency_ext(g,n,'in',l_re_in,conj)
    #output['out'] = update_frequency(g,n,'out',l_re_out)
    output['out'] = update_frequency_ext(g,n,'out',l_re_out,conj)
    output['pro'] = is_pronominal(g,n)
    output['giv'] = is_given(g,n)
    return output    

def score_core_roles(s_factors,e_di):
    """ smaller index, better score """
    output = float()
    for k in s_factors[e_di]:
        output += s_factors[e_di][k]/(int(k) + 1)
    return output

# SALIENCE formula
def salience(s_factors,w_dis,w_in,w_out,w_pro,w_giv): # recency excluded -> cache model
    """ -> salience score """
    output = sum([
        w_dis/s_factors['dis'],
        w_in*score_core_roles(s_factors,'in'),
        w_out*score_core_roles(s_factors,'out'),
        w_pro*s_factors['pro'],
        w_giv*s_factors['giv'],
        ])
    return output


#coreference chains and links
def update_coref_chains(text,i,ni,j,nj,wij):
    # text: list of AMR graphs
    # i/j: index of AMR graph containing anaphor ni/ antecedent nj
    # wij: weight of coreference link between ni and nj
    """ -> updated AMR text with coreference information """
    if 'coref' not in text[i].node[ni]:
        text[i].node[ni]['coref'] = list() # cannot be set()
        # because components' type is 'list' (unhashable)
    coref_link = (wij,j,nj)
    if 'coref' in text[j].node[nj]:
        for chain in text[j].node[nj]['coref']:
            chain.insert(0,coref_link)
            text[i].node[ni]['coref'].append(chain)
    else:
        text[i].node[ni]['coref'].append([coref_link])
        
def update_coref_links(link_dict,i,ni,j,nj,wij):
    # link_dict: default dict of resolved coreference links
    # i/j: index of AMR graph containing anaphor ni/ antecedent nj
    # wij: weight of coreference connection between ni and nj
    """ -> updated AMR text with coreference information """
    coref_link = (wij,j,nj)
    if ni in link_dict[i]:
        link_dict[i][ni].append(coref_link)
    else:
        link_dict[i][ni] = [coref_link]


def define_cache(text,cache_size,i):
    # text: list of AMR graphs, i: index of current AMR graph
    """ -> lists of indexes of  graphs in / out of cache"""
    if i>0:
        if i>cache_size:
            cache = [(i-x-1) for x in range(cache_size)]
            non_cache = range(i-cache_size-1,-1,-1)
        else:
            cache = range(i-1,-1,-1)
            non_cache = None
        return cache, non_cache
    return None
        
def rank_ancas(g):
    """ """
    output = dict()
    ancas = get_ancas(g)
    s_factors = dict()
    for n in ancas:
        if classify_node(g,n)!='conjunction':
            output[n] = salience(get_salience_factors(g,n),
                                 8,4,1,2,1)
        else:
            temp = set()
            for nn in g.node[n]['content'].next_:
                if re.match(l_re_op,nn.edge_label_) and \
                   (classify_node(g,nn.name_) not in \
                    {'event','special','constant'}):
                    temp.add(nn.name_)
            for nnn in temp:
                #output[nnn] = salience(get_salience_factors(g,nnn),
                output[nnn] = salience(get_salience_factors(g,nnn,n),
                                       8,4,1,2,1)
    return output

def new_concept(node):# node: node structure of amr_reader
    """ -> 'sure'/'unsure' """
    for next_node in node.next_:
        if (next_node.edge_label_==':mod' and \
           next_node.ful_name_ in {'another','any','some'}) or \
           (next_node.edge_label_==':quant' and \
           (next_node.ful_name_ in {'many','much','lot','few','little'} or \
            alnum_is(next_node.ful_name_)=='d')):
            return 'sure'
    return 'unsure'

def ne_constraint(n1,n2): # ne: named entity
    """ -> passed/failed """
    if n1.is_entity_ and n2.is_entity_ and \
       n1.entity_name_!=n2.entity_name_:# <-> string match
        return 'failed'
    return 'passed'
    
def string_match(anca_node,text,i,m_function,m_value,link_dict):
    # m_function/m_value: matching function/value
    """ -> set of antecas string-matching anca """
    output = set()
    if new_concept(anca_node)=='sure' or \
       anca_node.ful_name_=='thing': # anca_node.ful_name_ in PREDICATE_NOUNS
        return output
    g = text[i]
    ranked_ancas = rank_ancas(g)
    antecas = get_antecas(g)
    for n in antecas:
        n_node = g.node[n]['content'] 
        n_str = n_node.ful_name_
        if n_str in {'he','she','it','they'}:
            for cn in get_coref_nodes(text,i,n,link_dict):
                cn_node = text[cn[0]].node[cn[1]]['content']
                cn_str = cn_node.ful_name_
                if cn_str!=n_str: # not pronominal
                    n_str = cn_str
                    n_node = cn_node
                    break                    
        if m_function(anca_node.ful_name_,n_str)==m_value and \
           ne_constraint(anca_node,n_node)=='passed':
            output.add(n)
    #if any((n in ancas) for n in matched):
    if output.intersection(ranked_ancas):
        output = output.intersection(ranked_ancas)
        if len(output)>1:
            highest = max(ranked_ancas[n] for n in output)
            output = set(n for n in output if ranked_ancas[n]==highest)
    return output

def semantics_of_node(node):
    """ -> set of node name(s) and the corresponding POS"""
    output = set()
    #if node.ful_name_ not in PREDICATE_NOUNS:
    if node.ful_name_!='thing':
        pos = POS_SET
        output.add(node.ful_name_)
    else:
        pos = {'v'}
        for next_node in node.next_:
            if re.match(l_re_out,next_node.edge_label_):
##            if (node.ful_name_=='thing' and \
##                next_node.edge_label_==':ARG1-of') or \
##               (node.ful_name_!='thing' and \
##                next_node.edge_label_==':ARG0-of'):
                output.add(next_node.ful_name_.split('-')[0]) # event
    return output.difference(NOMINATIVE_PRONOUNS),pos

def get_sense_index_sum(pair): # pair: output of get_best_synset_pair()
    """ -> sum of sense indexes of two synsets in the pair """
    output = int()
    for s in pair[0]:
        output += int(s.name().split('.')[-1])
    return output    

def semantic_match(anca_node,text,i,
                   threshold,delta,sem_depth,sense_index_sum,
                   link_dict):
    # anca_node: node structure of amr_reader
    """ -> set of node names """
    output = set()
    if new_concept(anca_node)=='sure':
        return output
    words1,pos1 = semantics_of_node(anca_node)
    words2,pos2 = set(),set()
    g = text[i]
    ranked_ancas = rank_ancas(g)
    antecas = get_antecas(g)
    for n in antecas:
        n_node = text[i].node[n]['content']
        if n_node.ful_name_ in {'he','she','it','they'}:
            for cn in get_coref_nodes(text,i,n,link_dict):
                cn_node = text[cn[0]].node[cn[1]]['content']
                if cn_node.ful_name_!=n_node.ful_name_: # not pronominal
                    words2,pos2 = semantics_of_node(cn_node)
                    break
        else:            
            words2,pos2 = semantics_of_node(n_node)
        for w1 in words1:
            for w2 in words2:
##                if gender_constraint(w1,w2)=='passed' and \
##                   animacy_constraint(w1,w2)=='passed':
                if gender_constraint(w1,w2)=='passed' or \
                   animacy_constraint(w1,w2)=='passed':
                    semantic_pair = get_best_synset_pair(w1,w2,pos1,pos2)
                    temp_sim = threshold # lch_similarity
                    temp_dep = 3 # min_depth <- hard code
                    temp_sum = sense_index_sum
                    if pos1=={'v'} or pos2=={'v'} or \
                       (semantic_pair and semantic_pair[3] and \
                        semantic_pair[3]>sem_depth):
                        temp_sim = threshold - delta
                        temp_dep = 0 # <- hard code
                    if pos1=={'v'} or pos2=={'v'} or \
                       animacy(w1).intersection(w2)!=set():
                        temp_sum = sense_index_sum + 6 # bonus <- hard code
                    if semantic_pair and semantic_pair[1]>=temp_sim and \
                       semantic_pair[3] and semantic_pair[3]>temp_dep and \
                       ne_constraint(anca_node,n_node)=='passed' and \
                       get_sense_index_sum(semantic_pair)<=temp_sum:
                        output.add((semantic_pair[1],n_node.name_))
    if output:
        best_sem_sim = max(output)[0]
        output = set(e[1] for e in output if e[0]==best_sem_sim)
    if output.intersection(ranked_ancas):
        output = output.intersection(ranked_ancas)
        if len(output)>1:
            highest = max(ranked_ancas[n] for n in output)
            output = set(n for n in output if ranked_ancas[n]==highest)
    return output

def resolve_pro(p_node,g,threshold): # p_node: node of 3rd pronoun
    """ -> set of node names """
    ranked_ancas = rank_ancas(g)
    temp = [(n,r) for (n,r) in sorted(ranked_ancas.items(),
                                      key=itemgetter(1),reverse=True)
               if r>=threshold and \
                  not((g.node[n]['content'].ful_name_ in \
                       NOMINATIVE_PRONOUNS.difference({'one'})) and \
                      g.node[n]['content'].ful_name_!=p_node.ful_name_) and \
                  gender_constraint_pro(p_node.ful_name_,
                      g.node[n]['content'].ful_name_)=='passed' and \
                  animacy_constraint_pro(p_node.ful_name_,
                      g.node[n]['content'].ful_name_)=='passed']
    if not temp:
        output = set(n for n in get_antecas(g).difference(ranked_ancas)
                       if g.node[n]['content'].ful_name_==p_node.ful_name_)
        return output
    output = set(n for n,r in temp if r==temp[0][1])
    return output

def get_coref_nodes(text,i,n,link_dict):
    """ recursive function to get all nodes co-referred with n of text[i] """
    output = list()
    if (i in link_dict) and \
       (n in link_dict[i]) and \
       len(link_dict[i][n])==1:
        ante = link_dict[i][n][0][1:]
        output.append(ante)
        output.extend(get_coref_nodes(text,ante[0],ante[1],link_dict))
    return output

def resolve_anca(n,i,text,cache_info,link_dict):
    """ -> set of matched nodes """
    matched = set()
    threshold =[2.7,3.5]
    sem_depth = [5,3]
    sense_index_sum = [2,4]
    n_node = text[i].node[n]['content']
    if classify_node(text[i],n)=='other':        
        for p in range(len(cache_info)):
            if cache_info[p]: # for non_cache partition
                for j in cache_info[p]:
                    matched = string_match(n_node,text,j,
##                              truecase_matching_is,'f',link_dict)
                              lowercase_matching_is,'f',link_dict)
                    if matched:
                        matched = (j,matched)
                        break
                    else:
                        matched = semantic_match(n_node,text,j,
                                  threshold[p],0.6,sem_depth[p],
                                  sense_index_sum[p],link_dict)
                        if matched:
                            matched = (j,matched)
                            break
            if matched:
                break
    elif n_node.ful_name_ in {'i','we','you',"y'all"}:
        for j in range(i-1,-1,-1):
            matched = string_match(n_node,text,j,
                      truecase_matching_is,'f',link_dict)
            if matched:
                matched = (j,matched)
                break
    elif n_node.ful_name_ in {'he','she','it','they'}:
        for j in cache_info[0]:
            matched = resolve_pro(n_node,text[j],6.0)
            if matched:
                matched = (j,matched)
                break    
    return matched

l_re_op = r'^:op\d+$'
#def resolve_ancas_graph(i,text,cache_size):
def resolve_ancas_graph(i,text,cache_size,link_dict):
    """ -> dict whose keys are ancas and whose values are sets of antecas"""
    output = dict()
    cache_info = define_cache(text,cache_size,i)
    if cache_info:
        ranked_ancas = [n for n,_ in sorted(rank_ancas(text[i]).items(),
                                            key=itemgetter(1),
                                            reverse=True)]
        for n in ranked_ancas:
            output[n] = resolve_anca(n,i,text,cache_info,link_dict)
            if output[n]:
                w = 1/(len(output[n][1])) # weight of coref link
                for m in output[n][1]:
                    update_coref_chains(text,i,n,output[n][0],m,w) # <- this change rank values??? (<-> giveness???)
                    update_coref_links(link_dict,i,n,output[n][0],m,w)
    return output    
    
def resolve_ancas_text(text,cache_size):
    """ -> output (legacy) and link dict (key info) """
    output = dict()
    link_dict = dict()
    for i in range(len(text)):
        link_dict[i] = dict()
        output[i] = resolve_ancas_graph(i,text,cache_size,link_dict)
    return output,link_dict

def get_gold(f): # f: xml file of anaphora resolution annotation
    """ -> gold link dict """
    output = dict()
    tree = ET.parse(f)
    for (i,amr) in enumerate(tree.findall('./sntamr/amr')[1:]):
        output[i] = dict()
        if 'ana' in amr.attrib:
            for link in amr.attrib['ana'].split():
                n,jm = link.split(':')
                j,m = jm.split('.')
                output[i][n] = [(1.0,int(j)-2,m)]
    return output

def muc(gold_dict,test_dict):
    """ -> MUC F-measure """
    gold = float()
    test = float()
    correct = float()
    for i in gold_dict:
        gold += len(gold_dict[i])
        test += len(test_dict[i])
        for n in test_dict[i]:
            if n in gold_dict[i]:
                for l in test_dict[i][n]:
                    if l[1:]==gold_dict[i][n][0][1:]:
                        correct += l[0]
    print(gold,test,correct)
    r = correct/gold # recall
    p = correct/test # precision
    f1 = 2*r*p/(r+p)
    return r,p,f1
    

def main():
    """ """
    pass

if __name__ == "__main__":
    #main()
    amr_table = get_amr_table_path(DATA_AMR_EDITED)
    docids = ['Ariel','Aurora','Belle','Cinderella','Snow_White']
    text = dict()
    output = dict()
    link_dict = dict()
    print('Cache size: 1')
    for docid in docids:
        text[docid] = [AMRGraph(sen=s) for _,s in sorted(amr_table[docid].items())]
        output[docid],link_dict[docid] = resolve_ancas_text(text[docid],1)
        print(docid)
        for i in link_dict[docid]:            
            print(i,'\t',link_dict[docid][i])
        print('Number of nodes:', sum([(g.number_of_nodes()-1) for g in text[docid]]))
        print('Number of edges:', sum([(g.number_of_edges()-1) for g in text[docid]]))
        print('Number of ancas:', sum([len(output[docid][k]) for k in output[docid]]))
        print('Number of coref links:', sum([len(link_dict[docid][k]) for k in link_dict[docid]]))
        print('\n')
        
